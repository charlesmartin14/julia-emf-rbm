{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test implementing basic EMF iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:00:24.532500",
     "start_time": "2016-09-05T00:00:24.198487"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:00:24.953155",
     "start_time": "2016-09-05T00:00:24.534352"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.fixes import expit    \n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn import linear_model, datasets, metrics, preprocessing \n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:00:24.957669",
     "start_time": "2016-09-05T00:00:24.954681"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sig_means(x, b, W):\n",
    "    a = safe_sparse_dot(x, W.T) + b\n",
    "    return expit(a, out=a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use julia data set\n",
    "\n",
    "I don't know how to reproduce their normalization yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:00:28.743128",
     "start_time": "2016-09-05T00:00:24.959141"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('List of arrays in this file: \\n', [u'HDF5.name___X', u'HDF5.name___y'])\n",
      "(60000, 784) (60000,)\n",
      "norm of X  2117.63422548\n"
     ]
    }
   ],
   "source": [
    "hf =  h5py.File('mnist.h5','r')\n",
    "print('List of arrays in this file: \\n', hf.keys())\n",
    "X = np.array(hf.get('HDF5.name___X'))\n",
    "Y = np.array(hf.get('HDF5.name___y'))\n",
    "print X.shape, Y.shape\n",
    "hf.close()\n",
    "print \"norm of X \",np.linalg.norm(X,ord=2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-08-30T14:00:33.306668",
     "start_time": "2016-08-30T14:00:33.304102"
    }
   },
   "source": [
    "### Implement RBM and Test\n",
    "\n",
    "Eventually need to convert to sklearn code\n",
    "\n",
    "Using as much as existing RBM code as I can now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:00:28.751727",
     "start_time": "2016-09-05T00:00:28.745132"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.six.moves import xrange\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import gen_even_slices\n",
    "from sklearn.utils import issparse\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.utils.extmath import log_logistic\n",
    "from sklearn.utils.fixes import expit             # logistic function\n",
    "from sklearn.utils.validation import check_is_fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:00:28.760280",
     "start_time": "2016-09-05T00:00:28.753677"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_v_bias(X):\n",
    "    # If the user specifies the training dataset, it can be useful to                                                                                   \n",
    "    # initialize the visibile biases according to the empirical expected                                                                                \n",
    "    # feature values of the training data.                                                                                                              \n",
    "    #                                                                                                                                                   \n",
    "    # TODO: Generalize this biasing. Currently, the biasing is only written for                                                                         \n",
    "    #       the case of binary RBMs.\n",
    "    eps = 1e-8\n",
    "\n",
    "    probVis = np.mean(X,axis=0)             # Mean across  samples    \n",
    "    probVis[probVis < eps] = eps            # Some regularization (avoid Inf/NaN)  \n",
    "    print np.linalg.norm(probVis)\n",
    "\n",
    "    #probVis[probVis < (1.0-eps)] = (1.0-eps)   \n",
    "    v_bias = np.log(probVis / (1.0-probVis)) # Biasing as the log-proportion  \n",
    "    return v_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:00:28.802438",
     "start_time": "2016-09-05T00:00:28.762628"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EMF_RBM():\n",
    "    def __init__(self, n_components=256, learning_rate=0.005, batch_size=100,\n",
    "                 n_iter=20, verbose=0, random_state=None, momentum = 0.5, decay = 0.01, weight_decay='L1'):\n",
    "        self.n_components = n_components\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_iter = n_iter\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.momentum = momentum\n",
    "        self.decay = decay\n",
    "        self.weight_decay = weight_decay\n",
    "            \n",
    "        \n",
    "        # self.random_state_ = random_state\n",
    "        # always start with new random state\n",
    "        self.rng = check_random_state(random_state)\n",
    "        \n",
    "        # initialize arrays to 0\n",
    "        self.W = np.asarray(\n",
    "            self.rng.normal(\n",
    "                0,\n",
    "                0.001,\n",
    "                (self.n_components, X.shape[1])\n",
    "            ),\n",
    "            order='fortran')\n",
    "        \n",
    "        self.dW_prev = np.zeros_like(self.W)\n",
    "        self.W2 = self.W*self.W\n",
    "\n",
    "\n",
    "        self.h_bias = np.zeros(self.n_components, )\n",
    "        #self.v_bias = np.zeros(X.shape[1], )\n",
    "        self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n",
    "        \n",
    "        # learning rate / mini_batch\n",
    "        self.lr = 0.0\n",
    "        \n",
    "        print \"W norm \", np.linalg.norm(self.W, ord=2)\n",
    "        \n",
    "        # init vbias\n",
    "        self.v_bias = init_v_bias(X)\n",
    "        print \"v bias  \",self.v_bias.shape\n",
    "\n",
    "        print \"v bias norm \", np.linalg.norm(self.v_bias, ord=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:00:28.808961",
     "start_time": "2016-09-05T00:00:28.804760"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_layer(rbm, layer):\n",
    "    return (rbm.rng.random_sample(size=l.shape) < layer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:00:28.815103",
     "start_time": "2016-09-05T00:00:28.811310"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _sample_hiddens(rbm, v):\n",
    "    return sample_layer(rbm, _mean_hiddens(rbm, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:00:28.821949",
     "start_time": "2016-09-05T00:00:28.817476"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _mean_hiddens(rbm, v):\n",
    "    p = safe_sparse_dot(v, rbm.W.T) + rbm.h_bias\n",
    "    return expit(p, out=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:00:28.830168",
     "start_time": "2016-09-05T00:00:28.825374"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _mean_visibles(rbm, h):\n",
    "    p = np.dot(h, rbm.W) + rbm.v_bias\n",
    "    return expit(p, out=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:00:28.837142",
     "start_time": "2016-09-05T00:00:28.833546"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _sample_visibles(rbm, h):\n",
    "    return sample_layer(rbm, _mean_visible(rbm, h))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-02T19:24:39.829999",
     "start_time": "2016-09-02T19:24:39.826110"
    },
    "collapsed": true
   },
   "source": [
    "def sample_visibles(rbm, h):\n",
    "    p = np.dot(h, rbm.W)\n",
    "    p += rbm.v_bias\n",
    "    expit(p, out=p)\n",
    "    return (rbm.rng.random_sample(size=p.shape) < p), p"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-02T20:12:06.667567",
     "start_time": "2016-09-02T20:12:06.663179"
    },
    "collapsed": true
   },
   "source": [
    "def sample_hiddens(rbm, v):\n",
    "    p = _mean_hiddens(rbm, v)\n",
    "    return (rbm.rng.random_sample(size=p.shape) < p), p "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-02T19:24:40.295848",
     "start_time": "2016-09-02T19:24:40.292563"
    },
    "collapsed": true
   },
   "source": [
    "def gibbs_simple(rbm, v):\n",
    "    h_ = _sample_hiddens(rbm, v)\n",
    "    v_ = _sample_visibles(rbm, h_)\n",
    "    return v_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-02T19:24:41.043974",
     "start_time": "2016-09-02T19:24:41.034838"
    },
    "collapsed": false
   },
   "source": [
    "# uses sample_x(), not _sample_x()\n",
    "# quite different from simple\n",
    "# we return all samples, where as before we just sampled the visible\n",
    "# and the h we return is the mean, not the sample\n",
    "def gibbs(rbm, vis, n_times=1):\n",
    "    v_pos = vis\n",
    "    h_samp, h_pos = sample_hiddens(rbm, v_pos)\n",
    "    h_neg = np.zeros_like(h_pos)\n",
    "    v_neg = np.zeros_like(v_pos)\n",
    "            \n",
    "    if (n_times > 0):               \n",
    "        v_neg = _sample_visibles(rbm, h_samp)\n",
    "        h_samp, h_neg = sample_hiddens(rbm, v_neg)\n",
    "        \n",
    "        for i in range(n_times-1):\n",
    "            v_neg = sample_visibles(rbm, h_samp)\n",
    "            h_samp, h_neg = sample_hiddens(rbm, v_neg)\n",
    "        end\n",
    "    end\n",
    "    return v_pos, h_pos, v_neg, h_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:00:28.844646",
     "start_time": "2016-09-05T00:00:28.839172"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init starts with samples = [0] :| ?\n",
    "def init_batch(rbm, vis):\n",
    "    v_pos = vis\n",
    "    v_init = v_pos\n",
    "    \n",
    "    h_pos = _mean_hiddens(rbm, v_pos)\n",
    "    h_init = h_pos\n",
    "   \n",
    "    return v_pos, h_pos, v_init, h_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:00:28.857360",
     "start_time": "2016-09-05T00:00:28.846715"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit(rbm, X):\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    n_batches = int(np.ceil(float(n_samples) / rbm.batch_size))\n",
    "    \n",
    "    print \"fitting with n_batches = \",n_batches, \" in \",rbm.n_iter, \"iterations\"\n",
    "    \n",
    "    batch_slices = list(gen_even_slices(n_batches * rbm.batch_size,\n",
    "                                        n_batches, n_samples))\n",
    "    for iter in xrange(1, rbm.n_iter + 1):\n",
    "        for batch_slice in batch_slices:\n",
    "            fit_batch(rbm, X[batch_slice])\n",
    "\n",
    "        print iter , \" . \" ,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run an RBM right here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:00:28.866558",
     "start_time": "2016-09-05T00:00:28.859717"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def equilibrate(rbm, v0, h0, iters=5):\n",
    "    mv = v0\n",
    "    mh = h0\n",
    "    for i in range(iters):\n",
    "        mv = 0.5 *mv_update(rbm, mv, mh) + 0.5*mv\n",
    "        mh = 0.5 *mh_update(rbm, mv, mh) + 0.5*mh\n",
    "\n",
    "    return mv, mh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:00:28.874458",
     "start_time": "2016-09-05T00:00:28.868908"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mv_update(rbm, v, h):  \n",
    "    a = np.dot(h, rbm.W) + rbm.v_bias\n",
    "\n",
    "    h_fluc = h-(h*h)\n",
    "    a += h_fluc.dot(rbm.W2)*(0.5-v)\n",
    "    \n",
    "    return expit(a, out=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:00:28.881554",
     "start_time": "2016-09-05T00:00:28.876460"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mh_update(W2, v, h):\n",
    "    a = safe_sparse_dot(v, rbm.W.T) + rbm.h_bias\n",
    "    \n",
    "    v_fluc = v-(v*v)\n",
    "    a += v_fluc.dot(rbm.W2.T)*(0.5-h)\n",
    "\n",
    "    return expit(a, out=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:00:28.891953",
     "start_time": "2016-09-05T00:00:28.884046"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_gradient(rbm, v_pos, h_pos ,v_neg, h_neg):\n",
    "    # naive  / mean field\n",
    "    dW = safe_sparse_dot(v_pos.T, h_pos, dense_output=True).T - np.dot(h_neg.T, v_neg)\n",
    "    \n",
    "    #print \"dW naive\", np.linalg.norm(dW, ord=2)\n",
    "    # tap2 correction\n",
    "    h_fluc = (h_neg - (h_neg*h_neg)).T\n",
    "    v_fluc = (v_neg - (v_neg*v_neg))\n",
    "    dW -= h_fluc.dot(v_fluc)*rbm.W\n",
    "\n",
    "    return dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:00:28.927647",
     "start_time": "2016-09-05T00:00:28.893817"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_batch(rbm, X_batch):    \n",
    " \n",
    "    lr = float(rbm.learning_rate) / X_batch.shape[0]\n",
    "    decay = rbm.decay\n",
    "    \n",
    "    #print \"W, hb vb norm \", np.linalg.norm(rbm.W, ord=2), np.linalg.norm(rbm.h_bias, ord=2), np.linalg.norm(rbm.v_bias, ord=2)\n",
    "    #print \"batch norm \", np.linalg.norm(X_batch, ord=2)\n",
    "    \n",
    "    v_pos, h_pos, v_init, h_init = init_batch(rbm, X_batch)\n",
    " \n",
    "    #print \"v, h init norm \", np.linalg.norm(v_init, ord=2), np.linalg.norm(h_init, ord=2)\n",
    "    #print \"vb norm \"\n",
    "\n",
    "\n",
    "    # get_negative_samples\n",
    "    v_neg, h_neg = equilibrate(rbm, v_init, h_init) \n",
    "    #print \"v, h neg norm \", np.linalg.norm(v_neg, ord=2), np.linalg.norm(h_neg, ord=2)\n",
    "\n",
    "    # basic gradient\n",
    "    dW = weight_gradient(rbm, v_pos, h_pos ,v_neg, h_neg) \n",
    "    \n",
    "    #print \"dW norm \", np.linalg.norm(dW, ord=2)\n",
    "\n",
    "\n",
    "    # regularization based on weight decay\n",
    "    #  similar to momentum >\n",
    "    if rbm.weight_decay == \"L2\":\n",
    "        dW += decay * np.sign(rbm.W)\n",
    "    elif rbm.weight_decay == \"L1\":\n",
    "        dW += decay * rbm.W\n",
    "\n",
    "    # can we use BLAS here ?\n",
    "    # momentum\n",
    "    # note:  what do we do if lr changes per step ?    \n",
    "    dW = rbm.momentum * rbm.dW_prev  \n",
    "    rbm.W += lr * dW  \n",
    "    \n",
    "    # for next iteration\n",
    "    rbm.dW_prev =  dW  \n",
    "    rbm.W2 = rbm.W*rbm.W\n",
    "    \n",
    "    # update bias terms\n",
    "    rbm.h_bias += lr * (h_pos.sum(axis=0) - h_neg.sum(axis=0))\n",
    "    rbm.v_bias += lr * (np.asarray(v_pos.sum(axis=0)).squeeze() - v_neg.sum(axis=0))\n",
    "\n",
    "    # only resample (binomial) for CD\n",
    "    # h_neg[rbm.rng.uniform(size=h_neg.shape) < h_neg] = 1.0  \n",
    "    # rbm.h_samples_ = np.floor(h_neg, h_neg)\n",
    "        \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:00:29.007181",
     "start_time": "2016-09-05T00:00:28.929808"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W norm  0.0438319055922\n",
      "8.42502854466\n",
      "v bias   (784,)\n",
      "v bias norm  195.421791256\n"
     ]
    }
   ],
   "source": [
    "rbm = EMF_RBM(momentum=0.5, decay=0.01, learning_rate=0.005, n_iter=20)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-04T23:28:03.944844",
     "start_time": "2016-09-04T23:28:03.918453"
    },
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "fit_batch(rbm, X[0:100])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:05:55.912460",
     "start_time": "2016-09-05T00:00:30.660696"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting with n_batches =  600  in  20 iterations\n",
      "1  .  2  .  3  .  4  .  5  .  6  .  7  .  8  .  9  .  10  .  11  .  12  .  13  .  14  .  15  .  16  .  17  .  18  .  19  .  20  . \n"
     ]
    }
   ],
   "source": [
    "fit(rbm, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try classifier\n",
    "\n",
    "#### should we be using the EMF estimator?\n",
    "\n",
    "what are the correlations...do they drop to 0 as we converge ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:26:34.455630",
     "start_time": "2016-09-05T00:26:34.452408"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, datasets, metrics, preprocessing \n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:26:35.158747",
     "start_time": "2016-09-05T00:26:34.619915"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = sig_means(X, rbm.h_bias , rbm.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:26:35.165119",
     "start_time": "2016-09-05T00:26:35.160605"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 256) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print p.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:26:35.234387",
     "start_time": "2016-09-05T00:26:35.166771"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(p, Y, test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:27:12.888431",
     "start_time": "2016-09-05T00:26:35.236960"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 0.903833333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for c in [5000]:\n",
    "    lr  = linear_model.LogisticRegression()\n",
    "    lr.C = c\n",
    "    lr.fit(X_train, Y_train)\n",
    "    Y_test_pred = lr.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, Y_test_pred)\n",
    "\n",
    "    print c, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-09-05T00:27:12.893304",
     "start_time": "2016-09-05T00:27:12.890460"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### note bad, but not great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
